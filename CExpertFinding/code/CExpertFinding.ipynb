{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model was loaded!!\n",
      "0\n",
      "test q:\n",
      "[[221, 947, 2415, 2407], [1465, 440, 15713, 23579, 81929, 1353], [6982, 8489, 25533, -1, -1, -1]]\n",
      "experts:\n",
      "[ 0.32149917  0.6077112   0.3892125  ... -0.22167343 -0.12724012\n",
      " -0.06236279]\n",
      "1\n",
      "test q:\n",
      "[[726, 2077, 2457, 2460, 2707], [23293, 59817, 128445, 79], [68614, 68618, -1, -1]]\n",
      "experts:\n",
      "[ 0.56428325 -0.09417593 -0.5479343  ... -0.3292299  -0.24090874\n",
      " -0.2572034 ]\n",
      "2\n",
      "test q:\n",
      "[[230, 1053, 2456, 2630], [2422, 10, 23023, 152342], [7445, 7446, -1, -1]]\n",
      "experts:\n",
      "[ 0.30431357  0.16665891 -0.0557394  ... -0.20220482 -0.19128215\n",
      " -0.12911212]\n",
      "3\n",
      "test q:\n",
      "[[783, 2194, 2522, 2624], [96277, 205426, 30680, 1978], [119632, 166934, -1, -1]]\n",
      "experts:\n",
      "[ 0.43344396  0.5046733  -0.07980943 ... -0.1663596  -0.16467953\n",
      "  0.01391745]\n",
      "4\n",
      "test q:\n",
      "[[676, 1985, 2743, 2407, 2829], [23562, 9993, 5669, 18296], [52183, 55121, -1, -1]]\n",
      "experts:\n",
      "[ 0.2074053   0.05796319 -0.40580487 ... -0.17958373 -0.07406807\n",
      " -0.03331244]\n",
      "5\n",
      "test q:\n",
      "[[571, 1808, 2579, 2743], [12357, 8041, 20716, 5343, 693, 41884], [34000, 34011, 34015, -1, -1, -1]]\n",
      "experts:\n",
      "[ 0.1613537   0.13047618 -0.14663309 ... -0.4131062  -0.32247365\n",
      " -0.28070116]\n",
      "6\n",
      "test q:\n",
      "[[70, 1044, 2405, 2499], [270, 660, 16606, 15562], [1968, 1989, -1, -1]]\n",
      "experts:\n",
      "[ 0.96344274 -0.0291242  -0.33214688 ... -0.13050818 -0.16564375\n",
      "  0.24766856]\n",
      "7\n",
      "test q:\n",
      "[[386, 957, 2585], [3646, 9078, 5342, 1751], [17791, 17807, -1, -1]]\n",
      "experts:\n",
      "[ 0.38255754  0.38271433 -0.05845046 ... -0.04167068  0.04814476\n",
      "  0.11702764]\n",
      "8\n",
      "test q:\n",
      "[[694, 1391, 2397, 2683], [5342, 27231, 5390, 59638], [58204, 79196, -1, -1]]\n",
      "experts:\n",
      "[ 0.47634208  0.07188404  0.10027868 ...  0.05021799 -0.04452974\n",
      "  0.21718752]\n",
      "9\n",
      "test q:\n",
      "[[196, 1184, 2488, 2604], [10, 12946, 10787, 24915], [6056, 21200, -1, -1]]\n",
      "experts:\n",
      "[ 0.6559086   0.07835543 -0.04969531 ... -0.17640448  0.08558911\n",
      " -0.01426387]\n",
      "10\n",
      "test q:\n",
      "[[142, 1010, 2454, 2436, 2435], [219, 137, 5024, 1250], [4004, 4005, -1, -1]]\n",
      "experts:\n",
      "[ 0.14543104  0.47285235 -0.40526712 ... -0.3103758  -0.294356\n",
      " -0.23049957]\n",
      "11\n",
      "test q:\n",
      "[[157, 1149, 2477, 2577], [15, 2091, 8244, 57820], [4663, 4683, -1, -1]]\n",
      "experts:\n",
      "[ 0.7458168   0.3955791  -0.371714   ... -0.2116524  -0.07583112\n",
      " -0.13601696]\n",
      "12\n",
      "test q:\n",
      "[[280, 1085, 2505, 2540, 2552, 2400, 2506], [26, 13120, 2322, 137], [9576, 20228, -1, -1]]\n",
      "experts:\n",
      "[-0.0923779  -0.3458177  -0.409307   ... -0.47493374 -0.44762433\n",
      " -0.32195145]\n",
      "13\n",
      "test q:\n",
      "[[514, 1719, 2405, 2714, 2430], [15713, 4123, 14539, 33242], [27703, 27704, -1, -1]]\n",
      "experts:\n",
      "[ 0.10322541 -0.19147444 -0.4007311  ... -0.25439036 -0.22182018\n",
      " -0.15397674]\n",
      "14\n",
      "test q:\n",
      "[[43, 993, 2454, 2469], [386, 382, 2259, 7371, 26392, 22950], [1427, 1473, 4905, -1, -1, -1]]\n",
      "experts:\n",
      "[-0.18758011 -0.13358021  0.17762184 ... -0.29912788 -0.39982784\n",
      " -0.26597124]\n",
      "15\n",
      "test q:\n",
      "[[44, 918, 2470, 2471], [316, 10, 3009, 1926], [1383, 1388, -1, -1]]\n",
      "experts:\n",
      "[-0.03762817  0.2244665  -0.32938433 ... -0.20138669 -0.08076608\n",
      " -0.00051981]\n",
      "16\n",
      "test q:\n",
      "[[103, 1089, 2530, 2531, 2532], [3, 137, 152342, 7012], [2788, 3280, -1, -1]]\n",
      "experts:\n",
      "[ 0.7215847   1.1805358  -0.16818357 ... -0.10465544  0.17617708\n",
      " -0.31093383]\n",
      "17\n",
      "test q:\n",
      "[[7, 903, 2402, 2411], [10, 91, 725, 1756, 206006, 83603, 62445, 34732], [98, 122, 2157, 12257, -1, -1, -1, -1]]\n",
      "experts:\n",
      "[ 0.42088187  0.15870544 -0.35330975 ... -0.09789342 -0.31529588\n",
      " -0.12656838]\n",
      "18\n",
      "test q:\n",
      "[[26, 913, 2424, 2412, 2443], [45, 330, 1516, 482], [432, 2410, -1, -1]]\n",
      "experts:\n",
      "[ 0.02784377 -0.26178622 -0.30356508 ... -0.30470026 -0.33957767\n",
      " -0.00059378]\n",
      "19\n",
      "test q:\n",
      "[[192, 1053, 2395, 2415, 2599], [2433, 2274, 25096, 22736], [5958, 5963, -1, -1]]\n",
      "experts:\n",
      "[-0.04128671 -0.04544598 -0.39223158 ... -0.25775594 -0.21493399\n",
      " -0.14207113]\n",
      "20\n",
      "test q:\n",
      "[[71, 967, 2405, 2395, 2500], [531, 199, 10, 602, 219, 34012, 103265, 101133, 11209, 39896], [1973, 1974, 1987, 1998, 2010, -1, -1, -1, -1, -1]]\n",
      "experts:\n",
      "[-0.5297214  -0.46218348 -0.11796343 ... -0.47638166 -0.46565974\n",
      " -0.5547775 ]\n",
      "21\n",
      "test q:\n",
      "[[131, 1137, 2556, 2557], [1465, 912, 4188, 51737], [3625, 3861, -1, -1]]\n",
      "experts:\n",
      "[ 0.49632037  0.305543   -0.21574521 ...  0.03818828  0.11078382\n",
      "  0.08590204]\n",
      "22\n",
      "test q:\n",
      "[[502, 1698, 2501, 2751], [4123, 16575, 13651, 202974], [26736, 26739, -1, -1]]\n",
      "experts:\n",
      "[ 0.29572293  0.2686911  -0.42553258 ... -0.21957403 -0.20607162\n",
      " -0.06842661]\n",
      "23\n",
      "test q:\n",
      "[[495, 1466, 2690, 2748, 2402, 2436, 2579], [475, 1951, 64756, 21], [26002, 26003, -1, -1]]\n",
      "experts:\n",
      "[ 0.00861305 -0.02733886 -0.80470824 ... -0.5815842  -0.5405439\n",
      " -0.4463377 ]\n",
      "24\n",
      "test q:\n",
      "[[326, 1407, 2545, 2396, 2595, 2678], [7554, 793, 6484, 842], [12843, 12854, -1, -1]]\n",
      "experts:\n",
      "[-0.00067759 -0.14900088 -0.07624787 ... -0.48376012 -0.43128622\n",
      " -0.22554159]\n",
      "25\n",
      "test q:\n",
      "[[691, 1389, 2402, 2403, 2836], [17994, 64465, 14446, 50927], [56839, 149718, -1, -1]]\n",
      "experts:\n",
      "[ 0.64964986  0.62204665 -0.12393534 ... -0.24360836 -0.1958881\n",
      "  0.02373719]\n",
      "26\n",
      "test q:\n",
      "[[32, 970, 2415, 2455], [65, 197, 7585, 8292, 38760, 15575, 9544, 2713], [1107, 1118, 12827, 13838, -1, -1, -1, -1]]\n",
      "experts:\n",
      "[-0.08369005 -0.04043305 -0.03030562 ... -0.37972724 -0.31183493\n",
      " -0.35211247]\n",
      "27\n",
      "test q:\n",
      "[[9, 909, 2393], [21, 10, 140, 482, 1937, 3385, 7394, 37876, 1155, 142863], [165, 166, 264, 6010, 6576, -1, -1, -1, -1, -1]]\n",
      "experts:\n",
      "[-0.25837815 -0.02026057 -0.23732328 ... -0.36674333 -0.3555488\n",
      " -0.26218176]\n",
      "28\n",
      "test q:\n",
      "[[403, 1530, 2421, 2601, 2398], [37107, 65059, 20, 14958], [19123, 75061, -1, -1]]\n",
      "experts:\n",
      "[ 0.36081982 -0.17616439 -0.57016444 ... -0.391019   -0.3593886\n",
      " -0.0905146 ]\n",
      "29\n",
      "test q:\n",
      "[[334, 1414, 2405, 2406], [156, 7992, 3303, 10887, 30866, 106309], [13376, 13409, 21496, -1, -1, -1]]\n",
      "experts:\n",
      "[ 0.2800108  -0.05088997  0.04287779 ... -0.25254166 -0.247818\n",
      " -0.25221664]\n",
      "30\n",
      "test q:\n",
      "[[144, 1148, 2496, 2565], [1250, 15, 10040, 84686], [4114, 4115, -1, -1]]\n",
      "experts:\n",
      "[ 0.51478577  0.7435371  -0.39940393 ...  0.09162438 -0.18221253\n",
      " -0.05236888]\n",
      "31\n",
      "test q:\n",
      "[[42, 993, 2457, 2468], [251, 137, 158, 11044, 12677, 2322, 3808, 1518], [1362, 1368, 1407, 17614, -1, -1, -1, -1]]\n",
      "experts:\n",
      "[-0.5404142   0.04265797 -0.41242087 ... -0.72291243 -0.7764486\n",
      " -0.55112016]\n",
      "32\n",
      "test q:\n",
      "[[117, 1109, 2424, 2541, 2542], [1204, 1465, 63889, 75, 20400, 58715], [3421, 9934, 97214, -1, -1, -1]]\n",
      "experts:\n",
      "[ 0.27833402  0.21678203  0.09747076 ... -0.31485808 -0.3539256\n",
      " -0.25420678]\n",
      "33\n",
      "test q:\n",
      "[[21, 907, 2405, 2429], [137, 464, 3190, 25635], [365, 1576, -1, -1]]\n",
      "experts:\n",
      "[ 0.33902675  0.28908625 -0.28263533 ... -0.08446354  0.01939654\n",
      " -0.12207019]\n",
      "34\n",
      "test q:\n",
      "[[4, 885, 2402, 2403, 2404], [45, 50, 63833, 7169], [48, 51, -1, -1]]\n",
      "experts:\n",
      "[ 0.14987057 -0.11480927 -0.5257391  ... -0.37265652 -0.3655895\n",
      " -0.19078982]\n",
      "35\n",
      "test q:\n",
      "[[577, 882, 2481], [981, 23562, 30171, 23247], [34455, 34646, -1, -1]]\n",
      "experts:\n",
      "[ 0.42307106  0.1023221  -0.09102857 ... -0.04161286  0.01067066\n",
      "  0.07687676]\n",
      "36\n",
      "test q:\n",
      "[[224, 954, 2429, 2564], [594, 3522, 96277, 23023], [7100, 7106, -1, -1]]\n",
      "experts:\n",
      "[ 0.230851    0.3834909  -0.15325594 ... -0.15957206 -0.15735507\n",
      " -0.04486871]\n",
      "37\n",
      "test q:\n",
      "[[733, 928, 2546, 2442], [475, 63596, 3781, 15415], [72856, 72858, -1, -1]]\n",
      "experts:\n",
      "[ 0.3483603   0.57631856 -0.3575003  ... -0.15410423 -0.16141593\n",
      " -0.09292668]\n",
      "38\n",
      "test q:\n",
      "[[295, 1053, 2658, 2659], [170, 10, 1465, 3], [10096, 10101, -1, -1]]\n",
      "experts:\n",
      "[ 0.48237407  0.2986445  -0.03592658 ... -0.1338864  -0.12542021\n",
      " -0.04985988]\n",
      "39\n",
      "test q:\n",
      "[[828, 1882, 2405, 2393, 2394, 2873], [140905, 92056, 9052, 114824], [132763, 132849, -1, -1]]\n",
      "experts:\n",
      "[ 0.02800077  0.10571229 -0.45345604 ... -0.33452755 -0.3223951\n",
      " -0.2552145 ]\n",
      "40\n",
      "test q:\n",
      "[[24, 950, 2438, 2439, 2440], [33, 787, 2422, 82097, 19194, 594], [428, 3845, 9242, -1, -1, -1]]\n",
      "experts:\n",
      "[-0.5532026  -0.5954305  -0.14127034 ... -0.7314764  -0.6546267\n",
      " -0.66502297]\n",
      "41\n",
      "test q:\n",
      "[[133, 885, 2538, 2473, 2559], [1465, 12711, 119593, 2633], [3741, 19664, -1, -1]]\n",
      "experts:\n",
      "[ 0.56951517  0.43601745 -0.57990766 ... -0.30904132 -0.28799653\n",
      " -0.09287012]\n",
      "42\n",
      "test q:\n",
      "[[614, 1600, 2776, 2802], [7815, 31901, 44446, 307], [39549, 43350, -1, -1]]\n",
      "experts:\n",
      "[ 0.43747753  0.37467873 -0.01381922 ... -0.0195446   0.0745672\n",
      "  0.05943441]\n",
      "43\n",
      "test q:\n",
      "[[603, 918, 2796], [11209, 47030, 5342, 17476], [37835, 58288, -1, -1]]\n",
      "experts:\n",
      "[ 0.467493    0.43762252 -0.02407062 ...  0.01910377 -0.03587925\n",
      "  0.17859444]\n",
      "44\n",
      "test q:\n",
      "[[589, 1263, 2429, 2412, 2400], [11209, 16575, 6204, 7973], [35503, 35533, -1, -1]]\n",
      "experts:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17270598  0.3443394  -0.2750966  ... -0.1100812  -0.24201691\n",
      " -0.18896109]\n",
      "45\n",
      "test q:\n",
      "[[370, 1266, 2547, 2695], [10, 3, 267, 25084, 25160, 4332], [16317, 16347, 16527, -1, -1, -1]]\n",
      "experts:\n",
      "[ 0.3903349   0.92091084  0.60343456 ... -0.13787836 -0.15005201\n",
      " -0.14756799]\n",
      "46\n",
      "test q:\n",
      "[[175, 1189, 2513], [2103, 482, 13178, 5197], [5104, 5107, -1, -1]]\n",
      "experts:\n",
      "[ 0.29106736  0.43686736 -0.21441776 ...  0.09370559 -0.09064871\n",
      "  0.07838368]\n",
      "47\n",
      "test q:\n",
      "[[278, 1179, 2502, 2444], [10, 3522, 4934, 14154, 4934, 267], [9472, 9473, 9481, -1, -1, -1]]\n",
      "experts:\n",
      "[ 0.51480716  0.7881587  -0.11339295 ... -0.33934623 -0.36412948\n",
      " -0.16937971]\n",
      "48\n",
      "test q:\n",
      "[[490, 1684, 2698, 2586], [475, 15713, 13651, 10439], [25482, 25485, -1, -1]]\n",
      "experts:\n",
      "[ 0.3099176   0.24543831 -0.43861628 ... -0.15094924 -0.11890185\n",
      "  0.01370966]\n",
      "49\n",
      "test q:\n",
      "[[371, 1476, 2405, 2412, 2451, 2547, 2696], [1417, 10439, 9160, 84986, 39717, 9677], [16699, 16740, 16863, -1, -1, -1]]\n",
      "experts:\n",
      "[-0.5297942  -0.7360034  -0.5752666  ... -0.667632   -0.66262734\n",
      " -0.5782901 ]\n",
      "50\n",
      "test q:\n",
      "[[440, 1378, 2393, 2542], [1522, 7379, 7992, 103911], [21318, 21355, -1, -1]]\n",
      "experts:\n",
      "[ 0.06089932  0.2956894  -0.28099167 ... -0.01684934 -0.03876114\n",
      "  0.14274159]\n",
      "51\n",
      "test q:\n",
      "[[815, 2082, 2747, 2869], [44325, 145076, 205426, 18675], [125398, 133619, -1, -1]]\n",
      "experts:\n",
      "[ 0.46272004  0.3058781  -0.05361474 ... -0.23421645 -0.09512347\n",
      " -0.11653543]\n",
      "52\n",
      "test q:\n",
      "[[357, 1453, 2405, 2513, 2505], [8197, 404, 30767, 21343], [14960, 14982, -1, -1]]\n",
      "experts:\n",
      "[ 0.10635525 -0.00284237 -0.5498053  ... -0.25699365 -0.30265486\n",
      " -0.1452133 ]\n",
      "53\n",
      "test q:\n",
      "[[154, 1012, 2465, 2576], [1274, 482, 156, 15073, 3116, 1460, 15241, 7169], [4547, 4552, 4556, 23017, -1, -1, -1, -1]]\n",
      "experts:\n",
      "[-0.00134474  0.07519948  0.05120265 ... -0.38289666 -0.45862913\n",
      " -0.25126714]\n",
      "54\n",
      "test q:\n",
      "[[374, 1483, 2690, 2601], [10, 10923, 16575, 17235, 98, 8661], [17130, 17372, 37651, -1, -1, -1]]\n",
      "experts:\n",
      "[ 0.36822152  0.06492651  0.15023938 ... -0.29881585 -0.3201987\n",
      " -0.19020224]\n",
      "55\n",
      "test q:\n",
      "[[146, 1057, 2429, 2564], [137, 1816, 10787, 4892], [4188, 4225, -1, -1]]\n",
      "experts:\n",
      "[ 0.4978515   0.4178516   0.04358029 ...  0.09490061  0.03609359\n",
      " -0.02549165]\n",
      "56\n",
      "test q:\n",
      "[[795, 2219, 2637], [96277, 110415, 53438, 181887], [110940, 110942, -1, -1]]\n",
      "experts:\n",
      "[ 0.12933964  0.6248692  -0.18259835 ...  0.01140207  0.09509683\n",
      "  0.2488473 ]\n",
      "57\n",
      "test q:\n",
      "[[517, 993, 2412], [440, 4003, 3303, 81105, 503, 792], [27777, 27795, 27833, -1, -1, -1]]\n",
      "experts:\n",
      "[ 0.31119245 -0.01716477  0.10770881 ... -0.20275241 -0.050497\n",
      " -0.0597285 ]\n",
      "58\n",
      "test q:\n",
      "[[12, 919, 2418], [63, 135, 7425, 131553], [196, 226, -1, -1]]\n",
      "experts:\n",
      "[ 0.6097936   0.43992287 -0.05233586 ...  0.02354908  0.13877225\n",
      "  0.15494195]\n",
      "59\n",
      "test q:\n",
      "[[319, 1012, 2429, 2447], [3646, 52235, 4901, 188851], [12539, 62302, -1, -1]]\n",
      "experts:\n",
      "[ 0.3806861   0.19432044 -0.2934993  ... -0.17467928 -0.13380796\n",
      " -0.06364453]\n",
      "60\n",
      "test q:\n",
      "[[328, 1237, 2470, 2540, 2463, 2679], [475, 3646, 8561, 16389], [12964, 12978, -1, -1]]\n",
      "experts:\n",
      "[ 0.32031596 -0.01700342 -0.00290346 ... -0.12272453 -0.29752898\n",
      " -0.1536578 ]\n",
      "61\n",
      "test q:\n",
      "[[443, 1030, 2523, 2723, 2724], [8197, 13956, 121414, 61395], [21461, 21477, -1, -1]]\n",
      "experts:\n",
      "[ 0.01699352  0.05949366 -0.40929186 ... -0.29115587 -0.3171451\n",
      " -0.14565247]\n",
      "62\n",
      "test q:\n",
      "[[610, 1867, 2416, 2800], [27214, 16575, 12915, 23379], [38799, 38815, -1, -1]]\n",
      "experts:\n",
      "[-0.18982083  0.265222   -0.11108673 ... -0.07393873 -0.0993551\n",
      "  0.14182037]\n",
      "63\n",
      "test q:\n",
      "[[400, 1527, 2673, 2690, 2455], [1951, 3646, 9078, 12946, 5077, 23417], [18896, 18897, 18907, -1, -1, -1]]\n",
      "experts:\n",
      "[ 0.13259566 -0.14901245 -0.29491228 ... -0.46303415 -0.4103223\n",
      " -0.37293208]\n",
      "64\n",
      "test q:\n",
      "[[831, 2292, 2704], [44325, 208008, 11229, 2413], [152172, 168294, -1, -1]]\n",
      "experts:\n",
      "[ 0.5549009   0.65059614  0.04712474 ...  0.01248819 -0.02609926\n",
      "  0.08134401]\n",
      "65\n",
      "test q:\n",
      "[[498, 1691, 2579, 2743], [3009, 16575, 86024, 30734], [26235, 26506, -1, -1]]\n",
      "experts:\n",
      "[ 0.13536558  0.25953397  0.18334097 ... -0.19358277 -0.0882138\n",
      " -0.03052109]\n",
      "66\n",
      "test q:\n",
      "[[857, 2346, 2566, 2886, 2887], [209414, 209414, 85511, 33346], [174607, 174728, -1, -1]]\n",
      "experts:\n",
      "[ 0.01784706  0.01432133 -0.53834486 ... -0.2955469  -0.20096242\n",
      " -0.06919992]\n",
      "67\n",
      "test q:\n",
      "[[710, 1722, 2642], [7425, 27162, 155, 6181], [62989, 84656, -1, -1]]\n",
      "experts:\n",
      "[ 0.6215043   0.49192816 -0.00618684 ...  0.10048413  0.13730639\n",
      "  0.2810292 ]\n",
      "68\n",
      "test q:\n",
      "[[290, 1347, 2483, 2464], [5457, 11190, 7851, 137930], [9966, 34364, -1, -1]]\n",
      "experts:\n",
      "[-0.05703443  0.12407154 -0.11660635 ... -0.12310129 -0.10673881\n",
      "  0.10191655]\n",
      "69\n",
      "test q:\n",
      "[[198, 1037, 2465, 2418], [482, 1951, 334, 119408, 17286, 75037], [6089, 6090, 6181, -1, -1, -1]]\n",
      "experts:\n",
      "[ 0.1769304   0.2723151   0.05548859 ... -0.17484474 -0.07632935\n",
      "  0.00907046]\n",
      "70\n",
      "test q:\n",
      "[[527, 1740, 2458], [16265, 440, 13063, 5439], [28454, 28455, -1, -1]]\n",
      "experts:\n",
      "[ 0.70328164  0.98382473 -0.13588583 ...  0.07803237  0.03295827\n",
      "  0.12038291]\n",
      "71\n",
      "test q:\n",
      "[[344, 1430, 2687], [1417, 335, 3415, 53576, 44585, 16078], [14250, 14261, 14264, -1, -1, -1]]\n",
      "experts:\n",
      "[ 0.23574817  0.22106153  0.4007677  ... -0.20948482 -0.1555965\n",
      " -0.04855043]\n",
      "72\n",
      "test q:\n",
      "[[292, 1303, 2415, 2454], [5390, 156, 7394, 8661], [10024, 10027, -1, -1]]\n",
      "experts:\n",
      "[ 0.16754633  0.24223211 -0.00051385 ... -0.07452559 -0.15980184\n",
      "  0.05106395]\n",
      "73\n",
      "test q:\n",
      "[[671, 1391, 2697, 2615, 2824], [4003, 12442, 273966, 3394], [50498, 50608, -1, -1]]\n",
      "experts:\n",
      "[ 0.04231012  0.07253885 -0.63252616 ... -0.53598344 -0.53567684\n",
      " -0.4777187 ]\n",
      "74\n",
      "test q:\n",
      "[[97, 918, 2523, 2471, 2524], [196, 10, 20843, 23562], [2577, 2580, -1, -1]]\n",
      "experts:\n",
      "[ 0.10710657  0.06828964 -0.45027506 ... -0.33750784 -0.26297808\n",
      " -0.2548892 ]\n",
      "75\n",
      "test q:\n",
      "[[269, 947, 2405, 2629, 2648], [981, 5768, 12711, 58715], [9231, 10224, -1, -1]]\n",
      "experts:\n",
      "[ 0.41812488 -0.01648104 -0.4529413  ... -0.31406987 -0.28520238\n",
      " -0.18787837]\n",
      "76\n",
      "test q:\n",
      "[[271, 1259, 2490], [1465, 219, 6181, 9993], [9290, 9293, -1, -1]]\n",
      "experts:\n",
      "[ 0.6555953   0.34131765 -0.22714216 ... -0.00757301 -0.01894611\n",
      "  0.07206345]\n",
      "77\n",
      "test q:\n",
      "[[447, 1608, 2690, 2726, 2727], [3646, 14154, 256464, 209414], [21646, 22677, -1, -1]]\n",
      "experts:\n",
      "[ 0.3006169  -0.04725045 -0.48804665 ... -0.1944722  -0.11426342\n",
      " -0.0714643 ]\n",
      "78\n",
      "test q:\n",
      "[[722, 2066, 2435, 2516], [440, 12442, 2241, 54980], [67123, 67129, -1, -1]]\n",
      "experts:\n",
      "[ 0.32773516  0.16680259 -0.31243563 ... -0.0732522  -0.08319813\n",
      " -0.06552279]\n",
      "79\n",
      "test q:\n",
      "[[578, 1737, 2429, 2781, 2782], [16575, 17808, 57820, 8561], [34526, 38247, -1, -1]]\n",
      "experts:\n",
      "[ 0.42855102  0.19285691  0.18823159 ... -0.02041239 -0.12009197\n",
      " -0.04757237]\n",
      "80\n",
      "test q:\n",
      "[[420, 1562, 2543, 2433, 2479], [981, 24519, 423, 34694], [20152, 34709, -1, -1]]\n",
      "experts:\n",
      "[ 0.28699505  0.38391054 -0.40539646 ... -0.33932155 -0.1199981\n",
      " -0.27327037]\n",
      "81\n",
      "test q:\n",
      "[[23, 898, 2434, 2435, 2436, 2437], [116, 192, 37, 33346], [425, 2050, -1, -1]]\n",
      "experts:\n",
      "[-0.13927293 -0.22551316 -0.6858001  ... -0.3648944  -0.422786\n",
      " -0.32036978]\n",
      "82\n",
      "test q:\n",
      "[[598, 1847, 2508, 2792], [4003, 26125, 12442, 46332, 119593, 21310], [36294, 36853, 148288, -1, -1, -1]]\n",
      "experts:\n",
      "[-0.08016366  0.45763445  0.15922526 ... -0.22390842 -0.26562\n",
      " -0.21476614]\n",
      "83\n",
      "test q:\n",
      "[[46, 993, 2451, 2473], [412, 14080, 1448, 34732], [1505, 21647, -1, -1]]\n",
      "experts:\n",
      "[ 0.09689403  0.43339974 -0.08017123 ... -0.2590145  -0.20353705\n",
      " -0.09631747]\n",
      "84\n",
      "test q:\n",
      "[[108, 970, 2415, 2397, 2435, 2496], [3, 1455, 4843, 11416, 539, 6779, 27149, 142863, 11986, 27163], [2921, 3542, 8902, 18198, 18863, -1, -1, -1, -1, -1]]\n",
      "experts:\n",
      "[-0.3907088  -0.60761774 -0.6708063  ... -0.6160518  -0.7545692\n",
      " -0.4619254 ]\n",
      "85\n",
      "test q:\n",
      "[[409, 1433, 2712, 2713, 2510], [440, 22736, 15073, 131028], [19535, 32228, -1, -1]]\n",
      "experts:\n",
      "[ 0.45201045  0.21472722 -0.47563708 ... -0.33263975 -0.20799601\n",
      " -0.12492919]\n",
      "86\n",
      "test q:\n",
      "[[58, 1020, 2488], [10, 316, 29, 15134], [1746, 1747, -1, -1]]\n",
      "experts:\n",
      "[ 1.126958    0.5388233  -0.20523077 ...  0.03525525  0.30449378\n",
      "  0.1309247 ]\n",
      "87\n",
      "test q:\n",
      "[[438, 1594, 2435, 2436, 2617], [3646, 9535, 981, 2713], [21173, 21178, -1, -1]]\n",
      "experts:\n",
      "[ 0.2855012   0.03500158 -0.23693806 ... -0.15783703 -0.2039426\n",
      " -0.14126182]\n",
      "test_model done!!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#copyright etemadir@ryerson.ca Aug 2021\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from networkx import to_numpy_matrix\n",
    "import networkx as nx\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "try:\n",
    "    import ujson as json\n",
    "except:\n",
    "    import json\n",
    "import math\n",
    "from scipy.linalg import fractional_matrix_power\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import math\n",
    "\n",
    "class QRouting:    \n",
    "    \n",
    "    def  __init__(self,data):        \n",
    "        self.dataset=data\n",
    "        self.node_size=self.loadG()\n",
    "        \n",
    "    def init_model(self):\n",
    "                \n",
    "        #regression layer\n",
    "        self.regindim=2*11\n",
    "        self.W1=QRouting.weight_variable((self.regindim,8))\n",
    "        #self.W2=EndCold.weight_variable((self.W1.shape[1],8))\n",
    "        #self.W3=EndCold.weight_variable((self.W2.shape[1],16))\n",
    "        self.W4 = QRouting.weight_variable2(self.W1.shape[1])\n",
    "        #self.W4 = EndCold.weight_variable2(4*self.GCNW_2.shape[1])\n",
    "        self.b = tf.Variable(random.uniform(0, 1))\n",
    "        self.inputs=[]\n",
    "        self.outputs=[]    \n",
    "        \n",
    "        self.n_bins=11 #number of kernels\n",
    "        self.wordembedding_size=300        \n",
    "        self.lamb = 0.5\n",
    "\n",
    "        self.mus = QRouting.kernal_mus(self.n_bins, use_exact=True)\n",
    "        self.sigmas = QRouting.kernel_sigmas(self.n_bins, self.lamb, use_exact=True)\n",
    "        \n",
    "        self.wordembeddings = tf.Variable(tf.random.uniform([self.vocab_size+1, self.wordembedding_size], -1.0, 1.0,dtype=tf.float32),dtype=tf.float32)\n",
    "        \n",
    "        self.nodeembedding_size=128\n",
    "        self.nodeembeddings = tf.Variable(tf.random.uniform([self.node_size, self.nodeembedding_size], -1.0, 1.0,dtype=tf.float32),dtype=tf.float32)\n",
    "       \n",
    "        \n",
    "    def weight_variable(shape):\n",
    "        tmp = np.sqrt(6.0) / np.sqrt(shape[0]+shape[1])\n",
    "        initial = tf.random.uniform(shape, minval=-tmp, maxval=tmp)\n",
    "        return tf.Variable(initial,dtype=tf.float32)\n",
    "    \n",
    "    def weight_variable2(shape):\n",
    "        tmp = np.sqrt(6.0) / np.sqrt(shape)\n",
    "        initial = tf.random.uniform([shape,1], minval=-tmp, maxval=tmp)\n",
    "        return tf.Variable(initial,dtype=tf.float32)\n",
    "    \n",
    "    def loadG(self):        \n",
    "        fin=open(self.dataset+\"/CQG_proporties.txt\",\"r\")\n",
    "        N=int(fin.readline().strip().split(\" \")[2])\n",
    "        fin.close()\n",
    "        self.neighbors=[]\n",
    "        for i in range(N):\n",
    "            self.neighbors.append([i])\n",
    "        fin=open(self.dataset+\"/CQG.txt\",\"r\")\n",
    "        line=fin.readline().strip()\n",
    "        while line:\n",
    "            d=line.split(\" \")\n",
    "            s=int(d[0])\n",
    "            e=int(d[1])\n",
    "            self.neighbors[s].append(e)\n",
    "            self.neighbors[e].append(s)\n",
    "            line=fin.readline().strip()\n",
    "        fin.close()  \n",
    "        return N\n",
    "   \n",
    "    def load_traindata(self,qlen,alen):\n",
    "        \"\"\"load tain data\"\"\"\n",
    "        self.train_data=[]\n",
    "        self.train_label=[]\n",
    "        self.train_data_neighbors=[]\n",
    "        \n",
    "        INPUT=self.dataset+\"/train_data.txt\"\n",
    "        fin_train=open(INPUT)\n",
    "        INPUT2=self.dataset+\"/train_labels.txt\"\n",
    "        fin_label=open(INPUT2)\n",
    "        train=fin_train.readline().strip()\n",
    "        label=fin_label.readline().strip()\n",
    "        while train:\n",
    "            data=train.split(\" \")\n",
    "            lst=[]\n",
    "            for d in data:\n",
    "                lst.append(int(d)) \n",
    "            qid=lst[0]\n",
    "            answererid=lst[2]\n",
    "            qneighboirs=self.neighbors[qid].copy()\n",
    "            qneighboirs.remove(answererid)\n",
    "            eneigbors=self.neighbors[answererid].copy()\n",
    "            eneigbors.remove(qid)\n",
    "            self.train_data_neighbors.append( [qneighboirs,eneigbors]) \n",
    "            \n",
    "            #print(self.train_data_neighbors)\n",
    "            #sys.exit(0)\n",
    "            self.train_data.append(lst)\n",
    "            train=fin_train.readline().strip()\n",
    "            datal=float(label)\n",
    "            self.train_label.append(datal)\n",
    "            label=fin_label.readline().strip()\n",
    "        fin_train.close()\n",
    "        fin_label.close()\n",
    "        self.train_data=np.array(self.train_data)\n",
    "        self.train_data_neighbors=np.array(self.train_data_neighbors)\n",
    "        #self.train_label=np.array(self.train_label)\n",
    "        \n",
    "        #add nagetive samples\n",
    "        INPUT=self.dataset+\"/CQG_proporties.txt\"        \n",
    "        pfile=open(INPUT)\n",
    "        line=pfile.readline()\n",
    "        N=int(line.split(\" \")[2]) # number of nodes in the CQA network graph N=|Qestions|+|Askers|+|Answerers|+|tags|\n",
    "        line=pfile.readline()\n",
    "        qnum=int(line.split(\" \")[2])     \n",
    "        user_id_map={}\n",
    "        INPUT3=self.dataset+\"/user_id_map.txt\"\n",
    "        fin=open(INPUT3, \"r\",encoding=\"utf8\")\n",
    "        line=fin.readline().strip()\n",
    "        while line:            \n",
    "            e=line.split(\" \")\n",
    "            uname=\" \".join(e[1:])            \n",
    "            uname=int(uname.strip())\n",
    "            user_id_map[uname]=qnum+int(e[0])            \n",
    "            line=fin.readline().strip()\n",
    "        fin.close() \n",
    "        \n",
    "        \n",
    "        answerers=[]\n",
    "        INPUT=self.dataset+\"/user_answers.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                answerers.append(int(d[0]))\n",
    "        new_data=[]\n",
    "        new_data_neighbors=[]\n",
    "        ids=np.array([self.train_data[i][0] for i in range(self.train_data.shape[0])])\n",
    "        \n",
    "        \n",
    "        for i in set(ids): \n",
    "            #print(i)\n",
    "            ind=np.where(ids==i)\n",
    "            answerer_posids=[ a[2] for a in self.train_data[ind]]\n",
    "            #print(answerer_posids)\n",
    "            qaetinfo=self.train_data[ind][0].copy()\n",
    "            #print(qaetinfo)\n",
    "            qaetinfo[3]=-1\n",
    "            for kk in ind[0]:\n",
    "                \n",
    "                neid=user_id_map[random.choice(answerers)]\n",
    "                while neid in answerer_posids:\n",
    "                    neid=user_id_map[random.choice(answerers)]\n",
    "                #qaetinfo[2]=neid\n",
    "                p1=qaetinfo[0:2].copy()\n",
    "                p1.append(neid)\n",
    "                p1.extend(qaetinfo[3:])\n",
    "                \n",
    "                new_data.append([self.train_data[kk] , p1 ])\n",
    "                \n",
    "                qid=p1[0]\n",
    "                answererid=p1[2]\n",
    "                qneighboirs=self.neighbors[qid].copy()\n",
    "                \n",
    "                eneigbors=self.neighbors[answererid].copy()\n",
    "                          \n",
    "                new_data_neighbors.append([self.train_data_neighbors[kk] ,[qneighboirs,eneigbors]])\n",
    "            \n",
    "        self.train_data=np.array(new_data)\n",
    "        self.train_data_neighbors=np.array(new_data_neighbors)\n",
    "        #print(\"ok:\")\n",
    "        #print(self.train_data[0])\n",
    "        #print(self.train_data_neighbors[0])\n",
    "        self.train_label=np.array(self.train_label)\n",
    "        #print(self.train_data[-10:])\n",
    "        #sys.exit(0)\n",
    "        #end nagetive\n",
    "        \n",
    "        #print(self.train_label[:20])\n",
    "        #sys.exit(0)       \n",
    "        #shuffle\n",
    "        ind_new=[i for i in range(len(self.train_data))]\n",
    "        np.random.shuffle(ind_new)\n",
    "        self.train_data=self.train_data[ind_new,]        \n",
    "        self.train_data_neighbors=self.train_data_neighbors[ind_new,]\n",
    "        # load q and answer textself.train_data_neighbors\n",
    "        \n",
    "        self.qatext=[]\n",
    "        answers={}\n",
    "        qtitle={}\n",
    "        qcontent={}\n",
    "        vocab=[]\n",
    "        \n",
    "        INPUT=self.dataset+\"/vocab.txt\"\n",
    "        fin=open( INPUT, \"r\")\n",
    "        line=fin.readline()\n",
    "        line=fin.readline().strip()\n",
    "        while line:\n",
    "            v = line.split(\" \")        \n",
    "            vocab.append(v[0])\n",
    "            line=fin.readline().strip()\n",
    "        \n",
    "        INPUT=self.dataset+\"/A_content_nsw.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                answers[int(d[0])]=d[1:]\n",
    "        \n",
    "        INPUT=self.dataset+\"/Q_content_nsw.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                qcontent[int(d[0])]=d[1:]\n",
    "        \n",
    "        INPUT=self.dataset+\"/Q_title_nsw.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                qtitle[int(d[0])]=d[1:]        \n",
    "        \n",
    "        Q_id_map={}\n",
    "        INPUT2=self.dataset+\"/Q_id_map.txt\"\n",
    "        ids=np.loadtxt(INPUT2, dtype=int)\n",
    "        for e in ids:\n",
    "            Q_id_map[int(e[0])]=int(e[1])\n",
    "        \n",
    "        u_answers={}\n",
    "        INPUT=self.dataset+\"/user_answers.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                u_answers[user_id_map[int(d[0])]]=d[1::2]\n",
    "        \n",
    "        self.max_q_len=qlen\n",
    "        self.max_d_len=alen\n",
    "        self.vocab_size=len(vocab)\n",
    "        \n",
    "        delindx=0\n",
    "        delindexes=[]\n",
    "        for td in self.train_data:\n",
    "            #print(td)\n",
    "            qid=Q_id_map[td[0][0]]\n",
    "            #print(qid)\n",
    "            \n",
    "            aid=td[0][3] \n",
    "            #print(aid)\n",
    "            qtext=qtitle[qid].copy()\n",
    "            qtext.extend(qcontent[qid])            \n",
    "            qtext=qtext[:self.max_q_len]\n",
    "            #print(qtext)\n",
    "            qt=[]\n",
    "            for wr in qtext:\n",
    "                qt.append(vocab.index(wr)+1)\n",
    "            padzeros=self.max_q_len-len(qt)\n",
    "            #for zz in range(padzeros):\n",
    "                 #qt.append(0)\n",
    "            if aid!=-1:        \n",
    "                atext=answers[aid]\n",
    "                atext=atext[:self.max_d_len]\n",
    "                #print(atext)\n",
    "                at=[]\n",
    "                for wr in atext:\n",
    "                    if wr in vocab:\n",
    "                        at.append(vocab.index(wr)+1)\n",
    "                    else:\n",
    "                        print(str(wr)+\" not in  vocab\" )\n",
    "\n",
    "                padzeros=self.max_d_len-len(at)\n",
    "                #for zz in range(padzeros):\n",
    "                #     at.append(0)\n",
    "            else:\n",
    "                e=td[0][2]\n",
    "                etext1=[]\n",
    "                for aid in u_answers[int(e)]:\n",
    "                        #print(aid)\n",
    "                        etext1.extend(answers[int(aid)][:100])\n",
    "                        #etext1.extend(answers[int(aid)])\n",
    "                    \n",
    "                    #print(\"inter\")\n",
    "                    #print(inter)\n",
    "                etext=etext1\n",
    "                    #etext=etext1\n",
    "                if len(etext1)>self.max_d_len:                         \n",
    "                        etext=random.sample(etext1,self.max_d_len)\n",
    "                        \n",
    "                \n",
    "                #print(etext)\n",
    "                etext2=[]\n",
    "                for ii in range(len(etext)):\n",
    "                    if etext[ii] in vocab:\n",
    "                        etext2.append(vocab.index(etext[ii])+1)\n",
    "                    else:\n",
    "                        print(str(etext[ii])+\" not in  vocab\" )\n",
    "                at=etext2.copy()    \n",
    "            #self.qatext.append([qt,at])\n",
    "            if len(qt)==0 or len(at)==0:\n",
    "                delindexes.append(delindx)\n",
    "            else: \n",
    "                pos_txt=[qt,at]\n",
    "                \n",
    "            #print(td[1])\n",
    "            aid=td[1][3]\n",
    "            if aid!=-1:        \n",
    "                atext=answers[aid]\n",
    "                atext=atext[:self.max_d_len]\n",
    "                #print(atext)\n",
    "                at=[]\n",
    "                for wr in atext:\n",
    "                    if wr in vocab:\n",
    "                        at.append(vocab.index(wr)+1)\n",
    "                    else:\n",
    "                        print(str(wr)+\" not in  vocab\" )\n",
    "\n",
    "                padzeros=self.max_d_len-len(at)\n",
    "                #for zz in range(padzeros):\n",
    "                #     at.append(0)\n",
    "            else:\n",
    "                e=td[1][2]\n",
    "                etext1=[]\n",
    "                for aid in u_answers[int(e)]:\n",
    "                        #print(aid)\n",
    "                        etext1.extend(answers[int(aid)][:100])\n",
    "                        #etext1.extend(answers[int(aid)])\n",
    "                    \n",
    "                    #print(\"inter\")\n",
    "                    #print(inter)\n",
    "                etext=etext1\n",
    "                    #etext=etext1\n",
    "                if len(etext1)>self.max_d_len:                         \n",
    "                        etext=random.sample(etext1,self.max_d_len)\n",
    "                        \n",
    "                \n",
    "                #print(etext)\n",
    "                etext2=[]\n",
    "                for ii in range(len(etext)):\n",
    "                    if etext[ii] in vocab:\n",
    "                        etext2.append(vocab.index(etext[ii])+1)\n",
    "                    else:\n",
    "                        print(str(etext[ii])+\" not in  vocab\" )\n",
    "                at=etext2.copy()    \n",
    "            #self.qatext.append([qt,at])\n",
    "            if len(qt)==0 or len(at)==0 and delindx not in delindexes:\n",
    "                delindexes.append(delindx)\n",
    "            else: \n",
    "                neg_txt=[qt,at]\n",
    "            \n",
    "            \n",
    "            delindx+=1 \n",
    "            self.qatext.append([pos_txt,neg_txt])\n",
    "            #print( self.qatext)\n",
    "            #sys.exit(0)\n",
    "            \n",
    "        \n",
    "        self.qatext=np.array(self.qatext) \n",
    "#         print(self.qatext[:3])\n",
    "#         print(self.train_data[:3])\n",
    "#         print(delindexes)\n",
    "        if len(delindexes)!=0: #remove q with no answer\n",
    "            self.train_data=np.delete(self.train_data,delindexes)\n",
    "            self.train_label=np.delete(self.train_label, delindexes)\n",
    "            #self.qatext=np.delete(self.qatext,delindexes)\n",
    "#         print(self.qatext[:3])  \n",
    "#         print(self.train_data[:3])\n",
    "\n",
    "        self.val_data,self.val_data_neighbors,self.val_data_text=self.load_test()\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "    def load_test(self):\n",
    "        \"\"\"load test data for validation\"\"\"        \n",
    "        INPUT=self.dataset+\"/test_data.txt\"        \n",
    "        fin_test=open(INPUT)        \n",
    "        test=fin_test.readline().strip()\n",
    "        test_data=[]\n",
    "        \n",
    "        while test:\n",
    "            data=test.split(\";\")\n",
    "            lst=[]\n",
    "            for d in data[0].split(\" \"):\n",
    "                lst.append(int(d)) \n",
    "            \n",
    "            alst=[]\n",
    "            \n",
    "            for d in data[1].split(\" \")[0::3]:\n",
    "                alst.append(int(d))\n",
    "            \n",
    "            anlst=[]\n",
    "            for d in data[1].split(\" \")[1::3]:\n",
    "                anlst.append(int(d))\n",
    "            scoresanlst=[]\n",
    "            for d in data[1].split(\" \")[2::3]:\n",
    "                scoresanlst.append(int(d))\n",
    "            neg_e=[]\n",
    "            pos_e=[]\n",
    "            p_anlst=[]\n",
    "            for iii in range(len(anlst)):\n",
    "                if anlst[iii]==-1:\n",
    "                    neg_e.append(alst[iii])\n",
    "                else:\n",
    "                    pos_e.append(alst[iii])\n",
    "                    p_anlst.append(anlst[iii])\n",
    "                    \n",
    "            test_data.append([lst,alst,anlst,scoresanlst,pos_e,neg_e,p_anlst])\n",
    "            \n",
    "            test=fin_test.readline().strip()\n",
    "        fin_test.close()       \n",
    "        INPUT=self.dataset+\"/CQG_proporties.txt\"        \n",
    "        pfile=open(INPUT)\n",
    "        line=pfile.readline()\n",
    "        N=int(line.split(\" \")[2]) # number of nodes in the CQA network graph N=|Qestions|+|Askers|+|Answerers|+|tags|\n",
    "        line=pfile.readline()\n",
    "        qnum=int(line.split(\" \")[2])     \n",
    "        user_id_map={}\n",
    "        INPUT3=self.dataset+\"/user_id_map.txt\"\n",
    "        fin=open(INPUT3, \"r\",encoding=\"utf8\")\n",
    "        line=fin.readline().strip()\n",
    "        while line:            \n",
    "            e=line.split(\" \")\n",
    "            uname=\" \".join(e[1:])            \n",
    "            uname=uname.strip()\n",
    "            user_id_map[uname]=qnum+int(e[0])            \n",
    "            line=fin.readline().strip()\n",
    "        fin.close()    \n",
    "        answers={}\n",
    "        qtitle={}\n",
    "        qcontent={}\n",
    "        vocab=[]\n",
    "        INPUT=self.dataset+\"/vocab.txt\"\n",
    "        fin=open( INPUT, \"r\")\n",
    "        line=fin.readline()\n",
    "        line=fin.readline().strip()\n",
    "        while line:\n",
    "            v = line.split(\" \")        \n",
    "            vocab.append(v[0])\n",
    "            line=fin.readline().strip()\n",
    "        \n",
    "        INPUT=self.dataset+\"/A_content_nsw.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                answers[int(d[0])]=d[1:]\n",
    "        \n",
    "        INPUT=self.dataset+\"/Q_content_nsw.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                qcontent[int(d[0])]=d[1:]\n",
    "        \n",
    "        INPUT=self.dataset+\"/Q_title_nsw.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                qtitle[int(d[0])]=d[1:] \n",
    "        \n",
    "        Q_id_map_to_original={}\n",
    "        INPUT2=self.dataset+\"/Q_id_map.txt\"\n",
    "        ids=np.loadtxt(INPUT2, dtype=int)\n",
    "        for e in ids:\n",
    "            Q_id_map_to_original[int(e[0])]=int(e[1])\n",
    "            \n",
    "        max_q_len=20\n",
    "        max_d_len=100\n",
    "        u_answers={}\n",
    "        INPUT=self.dataset+\"/user_answers.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                u_answers[int(d[0])]=d[1::2]\n",
    "                \n",
    "        \n",
    "        batch_size=1     \n",
    "        #results=[]        \n",
    "        iii=0\n",
    "        val_data=[]\n",
    "        val_labels=[]\n",
    "        val_qatext=[]\n",
    "        val_data_neighbors=[]\n",
    "        for tq in test_data:\n",
    "            #print(iii)\n",
    "            iii=iii+1\n",
    "            #print(\"test q:\")\n",
    "            #print(tq)            \n",
    "           \n",
    "            ids=tq[1]  \n",
    "            \n",
    "            pos_e=tq[4]\n",
    "            neg_e=tq[5]\n",
    "            pos_e_answers=tq[6]\n",
    "            \n",
    "            answerids=tq[2]\n",
    "            scoresanlst=tq[3]\n",
    "            #print(\"experts:\")      \n",
    "            #print(ids)\n",
    "            inputs=[]\n",
    "            inputtext=[]\n",
    "            \n",
    "            qtext=[]\n",
    "            qid=Q_id_map_to_original[int(tq[0][0])]\n",
    "            qtext1=qtitle[qid].copy()\n",
    "            qtext1.extend(qcontent[qid])\n",
    "            qtext1=qtext1[:20]\n",
    "            qtext=qtext1.copy()\n",
    "            #print(qtext)\n",
    "            for i in range(len(qtext)):\n",
    "                qtext[i]=vocab.index(qtext[i])+1\n",
    "            \n",
    "            #if len(qtext)<max_q_len:                \n",
    "            #        for i in range(max_q_len-len(qtext)):\n",
    "                        #qtext.append(0)\n",
    "            kkk=0\n",
    "            for e1 in pos_e:  \n",
    "                e=int(e1)\n",
    "                answerid=pos_e_answers[kkk]\n",
    "                \n",
    "                etext1=[]\n",
    "                etext1=answers[int(answerid)][:100]\n",
    "                etext=etext1\n",
    "                \n",
    "                for ii in range(len(etext)):\n",
    "                    etext[ii]=vocab.index(etext[ii])+1\n",
    "                    \n",
    "                \n",
    "                testlst=tq[0][0:2]\n",
    "                testlst.append(user_id_map[str(e)])\n",
    "                testlst=np.concatenate((testlst,[answerid],tq[0][2:]))        \n",
    "                \n",
    "                \n",
    "                \n",
    "                qid=testlst[0]\n",
    "                answererid=testlst[2]\n",
    "                qneighboirs=self.neighbors[qid].copy()\n",
    "                #qneighboirs.remove(answererid)\n",
    "                pos_eneigbors=self.neighbors[answererid].copy()\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #negative sample \n",
    "                e_neg=int(neg_e[kkk]) #get one of the negetive experts\n",
    "                                \n",
    "                etext1=[]    \n",
    "                for aid in u_answers[int(e_neg)]:\n",
    "                    #print(aid)\n",
    "                    etext1.extend(answers[int(aid)][:100])\n",
    "                    #etext1.extend(answers[int(aid)])\n",
    "\n",
    "                #print(\"inter\")\n",
    "                #print(inter)\n",
    "                etext_neg=etext1\n",
    "                #etext=etext1\n",
    "                if len(etext1)>max_d_len:                         \n",
    "                        etext_neg=random.sample(etext1,max_d_len)\n",
    "                        \n",
    "                                \n",
    "                for ii in range(len(etext_neg)):\n",
    "                    etext_neg[ii]=vocab.index(etext_neg[ii])+1\n",
    "                \n",
    "                #if len(etext)<max_d_len:                \n",
    "                    #for i in range(max_d_len-len(etext)):\n",
    "                        #etext.append(0)\n",
    "                \n",
    "                testlst_neg=tq[0][0:2]\n",
    "                testlst_neg.append(user_id_map[str(e_neg)])\n",
    "                testlst_neg=np.concatenate((testlst_neg,[-1],tq[0][2:]))        \n",
    "                \n",
    "                answererid=testlst_neg[2]                \n",
    "                neg_eneigbors=self.neighbors[answererid].copy()\n",
    "                  \n",
    "                \n",
    "                val_data.append([testlst,testlst_neg])\n",
    "                val_data_neighbors.append([[qneighboirs,pos_eneigbors],[qneighboirs,neg_eneigbors]])\n",
    "                val_qatext.append([[qtext,etext],[qtext,etext_neg]])\n",
    "                \n",
    "                kkk+=1 \n",
    "        return np.array(val_data),np.array(val_data_neighbors), np.array(val_qatext)   \n",
    "    \n",
    "    #adopted from knrm paper ref:https://github.com/AdeDZY/K-NRM\n",
    "    @staticmethod\n",
    "    def kernal_mus(n_kernels, use_exact):\n",
    "        \"\"\"\n",
    "        get the mu for each guassian kernel. Mu is the middle of each bin\n",
    "        :param n_kernels: number of kernels (including exact match). first one is exact match\n",
    "        :return: l_mu, a list of mu.\n",
    "        \"\"\"\n",
    "        if use_exact:\n",
    "            l_mu = [1]\n",
    "        else:\n",
    "            l_mu = [2]\n",
    "        if n_kernels == 1:\n",
    "            return l_mu\n",
    "\n",
    "        bin_size = 2.0 / (n_kernels - 1)  # score range from [-1, 1]\n",
    "        l_mu.append(1 - bin_size / 2)  # mu: middle of the bin\n",
    "        for i in range(1, n_kernels - 1):\n",
    "            l_mu.append(l_mu[i] - bin_size)\n",
    "        return l_mu\n",
    "\n",
    "    #adopted from knrm paper copied from knrm paper ref:https://github.com/AdeDZY/K-NRM\n",
    "    @staticmethod\n",
    "    def kernel_sigmas(n_kernels, lamb, use_exact):\n",
    "        \"\"\"\n",
    "        get sigmas for each guassian kernel.\n",
    "        :param n_kernels: number of kernels (including exactmath.)\n",
    "        :param lamb:\n",
    "        :param use_exact:\n",
    "        :return: l_sigma, a list of simga\n",
    "        \"\"\"\n",
    "        bin_size = 2.0 / (n_kernels - 1)\n",
    "        l_sigma = [0.00001]  # for exact match. small variance -> exact match\n",
    "        if n_kernels == 1:\n",
    "            return l_sigma\n",
    "\n",
    "        l_sigma += [bin_size * lamb] * (n_kernels - 1)\n",
    "        return l_sigma\n",
    "    \n",
    "    def q_a_rbf_words(self,inputs_q,inputs_d):  \n",
    "        \"\"\"text encoder \\Psi\"\"\"\n",
    "        # look up embeddings for each term. [nbatch, qlen, emb_dim]\n",
    "        self.max_q_len=len(inputs_q[0])\n",
    "        self.max_d_len=len(inputs_d[0])\n",
    "        \n",
    "        q_embed = tf.nn.embedding_lookup(self.wordembeddings, inputs_q, name='qemb')\n",
    "        d_embed = tf.nn.embedding_lookup(self.wordembeddings, inputs_d, name='demb')\n",
    "        batch_size=1\n",
    "        \n",
    "        # normalize and compute similarity matrix using l2 norm         \n",
    "        norm_q = tf.sqrt(tf.reduce_sum(tf.square(q_embed), 2))\n",
    "        #print(norm_q)\n",
    "        norm_q=tf.reshape(norm_q,(len(norm_q),len(norm_q[0]),1))\n",
    "        #print(norm_q)\n",
    "        normalized_q_embed = q_embed / norm_q\n",
    "        #print(normalized_q_embed)\n",
    "        norm_d = tf.sqrt(tf.reduce_sum(tf.square(d_embed), 2))\n",
    "        norm_d=tf.reshape(norm_d,(len(norm_d),len(norm_d[0]),1))\n",
    "        normalized_d_embed = d_embed / norm_d\n",
    "        #print(normalized_d_embed)\n",
    "        tmp = tf.transpose(normalized_d_embed, perm=[0, 2, 1])\n",
    "        #print(tmp)\n",
    "        sim =tf.matmul(normalized_q_embed, tmp)\n",
    "        #print(sim)        \n",
    "        # compute gaussian kernel\n",
    "        rs_sim = tf.reshape(sim, [batch_size, self.max_q_len, self.max_d_len, 1])\n",
    "        #print(rs_sim)\n",
    "        \n",
    "        tmp = tf.exp(-tf.square(tf.subtract(rs_sim, self.mus)) / (tf.multiply(tf.square(self.sigmas), 2)))\n",
    "        #print(tmp)\n",
    "        \n",
    "        feats = []  # store the soft-TF features from each field.\n",
    "        # sum up gaussian scores\n",
    "        kde = tf.reduce_sum(tmp, [2])\n",
    "        kde = tf.math.log(tf.maximum(kde, 1e-10)) * 0.01  # 0.01 used to scale down the data.\n",
    "        # [batch_size, qlen, n_bins]\n",
    "        \n",
    "        #print(kde)\n",
    "        # aggregated query terms\n",
    "        # q_weights = [1, 1, 0, 0...]. Works as a query word mask.\n",
    "        # Support query-term weigting if set to continous values (e.g. IDF).\n",
    "        \n",
    "        #q_weights=np.where(np.array(inputs_q)>0,1,0)\n",
    "        #q_weights=tf.dtypes.cast(q_weights, tf.float32)\n",
    "        #q_weights = tf.reshape(q_weights, shape=[batch_size, self.max_q_len, 1])\n",
    "        \n",
    "        aggregated_kde = tf.reduce_sum(kde , [1])  # [batch, n_bins]   *q_weights\n",
    "        #print( aggregated_kde)\n",
    "        feats.append(aggregated_kde) # [[batch, nbins]]\n",
    "        feats_tmp = tf.concat( feats,1)  # [batch, n_bins]\n",
    "        #print (\"batch feature shape:\", feats_tmp.get_shape())\n",
    "        \n",
    "        # Reshape. (maybe not necessary...)\n",
    "        feats_flat = tf.reshape(feats_tmp, [-1, self.n_bins])\n",
    "        feats_flat2=tf.reshape(feats_flat, [1,self.n_bins])\n",
    "        return(feats_flat2)  \n",
    "    \n",
    "    def q_a_rbf_nodes(self,inputs_q,inputs_d): \n",
    "        \"\"\"sub-graph encoder \\Phi\"\"\"        \n",
    "        # look up embeddings for each term. [nbatch, qlen, emb_dim]\n",
    "        self.max_q_len=len(inputs_q[0])\n",
    "        self.max_d_len=len(inputs_d[0])\n",
    "        \n",
    "        q_embed = tf.nn.embedding_lookup(self.nodeembeddings, inputs_q, name='qemb')\n",
    "        d_embed = tf.nn.embedding_lookup(self.nodeembeddings, inputs_d, name='demb')\n",
    "        batch_size=1\n",
    "        \n",
    "        # normalize and compute similarity matrix using l2 norm         \n",
    "        norm_q = tf.sqrt(tf.reduce_sum(tf.square(q_embed), 2))\n",
    "        #print(norm_q)\n",
    "        norm_q=tf.reshape(norm_q,(len(norm_q),len(norm_q[0]),1))\n",
    "        #print(norm_q)\n",
    "        normalized_q_embed = q_embed / norm_q\n",
    "        #print(normalized_q_embed)\n",
    "        norm_d = tf.sqrt(tf.reduce_sum(tf.square(d_embed), 2))\n",
    "        norm_d=tf.reshape(norm_d,(len(norm_d),len(norm_d[0]),1))\n",
    "        normalized_d_embed = d_embed / norm_d\n",
    "        #print(normalized_d_embed)\n",
    "        tmp = tf.transpose(normalized_d_embed, perm=[0, 2, 1])\n",
    "        #print(tmp)\n",
    "        sim =tf.matmul(normalized_q_embed, tmp)\n",
    "        #print(sim)        \n",
    "        # compute gaussian kernel\n",
    "        rs_sim = tf.reshape(sim, [batch_size, self.max_q_len, self.max_d_len, 1])\n",
    "        #print(rs_sim)\n",
    "        \n",
    "        tmp = tf.exp(-tf.square(tf.subtract(rs_sim, self.mus)) / (tf.multiply(tf.square(self.sigmas), 2)))\n",
    "        #print(tmp)\n",
    "        \n",
    "        feats = []  # store the soft-TF features from each field.\n",
    "        # sum up gaussian scores\n",
    "        kde = tf.reduce_sum(tmp, [2])\n",
    "        kde = tf.math.log(tf.maximum(kde, 1e-10)) * 0.01  # 0.01 used to scale down the data.\n",
    "        # [batch_size, qlen, n_bins]\n",
    "        \n",
    "        #print(kde)\n",
    "        # aggregated query terms\n",
    "        # q_weights = [1, 1, 0, 0...]. Works as a query word mask.\n",
    "        # Support query-term weigting if set to continous values (e.g. IDF).\n",
    "        \n",
    "        #q_weights=np.where(np.array(inputs_q)>0,1,0)\n",
    "        #q_weights=tf.dtypes.cast(q_weights, tf.float32)\n",
    "        #q_weights = tf.reshape(q_weights, shape=[batch_size, self.max_q_len, 1])\n",
    "        \n",
    "        aggregated_kde = tf.reduce_sum(kde , [1])  # [batch, n_bins]   *q_weights\n",
    "        #print( aggregated_kde)\n",
    "        feats.append(aggregated_kde) # [[batch, nbins]]\n",
    "        feats_tmp = tf.concat( feats,1)  # [batch, n_bins]\n",
    "        #print (\"batch feature shape:\", feats_tmp.get_shape())\n",
    "        \n",
    "        # Reshape. (maybe not necessary...)\n",
    "        feats_flat = tf.reshape(feats_tmp, [-1, self.n_bins])\n",
    "        feats_flat2=tf.reshape(feats_flat, [1,self.n_bins])\n",
    "        return(feats_flat2)\n",
    "        \n",
    "               \n",
    "    def model_test(self):\n",
    "        embed=[]\n",
    "        #print(self.inputs)\n",
    "        for k in range(len(self.inputs)): \n",
    "            ind=self.inputs[k]\n",
    "            qtext=[self.qatextinput[k][0]]\n",
    "            atext=[self.qatextinput[k][1]]\n",
    "            #print(qtext)\n",
    "            #print(atext)\n",
    "            q_a_rbf_words=self.q_a_rbf_words(qtext,atext)\n",
    "            \n",
    "            q_neighbors=[self.qaneighborinput[k][0]]\n",
    "            a_neighbors=[self.qaneighborinput[k][1]]\n",
    "            \n",
    "            q_a_rbf_nodes=self.q_a_rbf_nodes(q_neighbors,a_neighbors)\n",
    "                     \n",
    "            embed1=tf.concat([q_a_rbf_nodes,q_a_rbf_words],1, name='concat')\n",
    "            #embed1=tf.concat([qembed,askerembed,answererembed,tagsembed],1, name='concat')\n",
    "            embed.append(embed1)\n",
    "            \n",
    "        embed=tf.reshape(embed,[len(self.inputs),self.regindim])    \n",
    "        #return  tf.reshape(tf.matmul(embed,self.W4),[len(self.inputs)]) + self.b\n",
    "        #print(embed)\n",
    "        #print(len(embed))\n",
    "        #print(len(embed[0]))\n",
    "        w1out=tf.nn.tanh(tf.matmul(embed,self.W1))\n",
    "        #print(w1out.shape)\n",
    "        #w2out=tf.nn.tanh(tf.matmul(w1out,self.W2))\n",
    "        #print(w2out.shape)\n",
    "        #w3out=tf.nn.tanh(tf.matmul(w2out,self.W3))\n",
    "        #print(w3out.shape)   \n",
    "        return  tf.reshape(tf.matmul(w1out,self.W4),[len(self.inputs)]) + self.b\n",
    "    \n",
    "    def model(self,inputs,qatextinput,qaneighborinput):\n",
    "        embed=[]\n",
    "        #print(self.inputs)\n",
    "        for k in range(len(inputs)): \n",
    "            ind=inputs[k]\n",
    "            qtext=[qatextinput[k][0]]\n",
    "            #print(qtext)\n",
    "            atext=[qatextinput[k][1]]\n",
    "            #print(atext)\n",
    "            #print(qtext)\n",
    "            #print(atext)\n",
    "            #sys.exit(0)\n",
    "            q_a_rbf_words=self.q_a_rbf_words(qtext,atext)\n",
    "            \n",
    "            q_neighbors=[qaneighborinput[k][0]]\n",
    "            a_neighbors=[qaneighborinput[k][1]]\n",
    "            \n",
    "            q_a_rbf_nodes=self.q_a_rbf_nodes(q_neighbors,a_neighbors)\n",
    "                     \n",
    "            embed1=tf.concat([q_a_rbf_nodes,q_a_rbf_words],1, name='concat')\n",
    "            #embed1=tf.concat([qembed,askerembed,answererembed,tagsembed],1, name='concat')\n",
    "            embed.append(embed1)\n",
    "        embed=tf.reshape(embed,[len(self.inputs),self.regindim])    \n",
    "        #return  tf.reshape(tf.matmul(embed,self.W4),[len(self.inputs)]) + self.b\n",
    "        #print(embed)\n",
    "        #print(len(embed))\n",
    "        #print(len(embed[0]))\n",
    "        w1out=tf.nn.tanh(tf.matmul(embed,self.W1))\n",
    "        #print(w1out.shape)\n",
    "        #w2out=tf.nn.tanh(tf.matmul(w1out,self.W2))\n",
    "        #print(w2out.shape)\n",
    "        #w3out=tf.nn.tanh(tf.matmul(w2out,self.W3))\n",
    "        #print(w3out.shape)   \n",
    "        return  tf.reshape(tf.matmul(w1out,self.W4),[len(self.inputs)]) + self.b\n",
    "        \n",
    "    \n",
    "    def loss(self):\n",
    "        self.L= tf.reduce_mean(tf.math.maximum(0,1-self.model(self.inputs,self.qatextinput,self.qaneighborinput) \n",
    "                                         + self.model(self.inputs_neg,self.qatextinput_neg,self.qaneighborinput_neg)))\n",
    "        return self.L  \n",
    "        \n",
    "    def train(self,modelname): \n",
    "        self.load_traindata(20,100)\n",
    "        self.init_model()        \n",
    "        print(\"train data loaded!!\")     \n",
    "        len_train_data=len(self.train_data)\n",
    "        val_len=len(self.val_data)\n",
    "        loss_=0\n",
    "        epochs = range(50)\n",
    "        self.batch_size=4\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        decayed_lr = tf.compat.v1.train.exponential_decay(0.0001,\n",
    "                                        global_step, 700,\n",
    "                                        0.95, staircase=True)\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=decayed_lr,epsilon=6e-7)#(decayed_lr,epsilon=5e-6)\n",
    "        logfile=open(self.dataset+\"/results/log.txt\",\"w\")\n",
    "        t_loss=[]\n",
    "        v_loss=[]\n",
    "        eps=[]\n",
    "        \n",
    "        for epoch in epochs:\n",
    "            ind_new=[i for i in range(len_train_data)]\n",
    "            np.random.shuffle(ind_new)\n",
    "            self.train_data=self.train_data[ind_new,]\n",
    "            self.train_label=self.train_label[ind_new,]           \n",
    "            self.qatext=self.qatext[ind_new,]  \n",
    "            self.train_data_neighbors=self.train_data_neighbors[ind_new,] \n",
    "            \n",
    "            start=0\n",
    "            end=0\n",
    "            for i in range(math.ceil(len_train_data/self.batch_size)):\n",
    "                if ((i+1)*self.batch_size)<len_train_data:                    \n",
    "                    start=i*self.batch_size\n",
    "                    end=(i+1)*self.batch_size\n",
    "                else:                    \n",
    "                    start=i*self.batch_size\n",
    "                    end=len_train_data\n",
    "                    \n",
    "                self.inputs= [self.train_data[start:end][i][0] for i in range(len(self.train_data[start:end]))]                            \n",
    "                self.qatextinput=[self.qatext[start:end][i][0] for i in range(len(self.qatext[start:end]))]\n",
    "                self.qaneighborinput=[self.train_data_neighbors[start:end][i][0] for i in range(len( self.train_data_neighbors[start:end]))]\n",
    "                \n",
    "                self.inputs_neg= [self.train_data[start:end][i][1] for i in range(len(self.train_data[start:end]))]                            \n",
    "                self.qatextinput_neg=[self.qatext[start:end][i][1] for i in range(len(self.qatext[start:end]))]\n",
    "                self.qaneighborinput_neg=[self.train_data_neighbors[start:end][i][1] for i in range(len( self.train_data_neighbors[start:end]))]\n",
    "                                             \n",
    "                #print(i)\n",
    "                #print(self.inputs)\n",
    "                #print(self.outputs)\n",
    "                #print(self.model())\n",
    "                #print(self.qatextinput)\n",
    "                \n",
    "                opt.minimize(self.loss, var_list=[self.W1,self.W4,self.b,self.wordembeddings,self.nodeembeddings])#,self.W2,self.W3\n",
    "                \n",
    "                #q_embed = tf.nn.embedding_lookup(self.embeddings, self.qatextinput[0][0], name='qemb')\n",
    "                #print(self.qatextinput[0][1])\n",
    "                #print(self.outputs)\n",
    "                #d_embed = tf.nn.embedding_lookup(self.embeddings, self.qatextinput[0][1], name='demb')\n",
    "                #print(self.embeddings[0,:10])\n",
    "                \n",
    "                loss_+=self.L \n",
    "                \n",
    "                global_step.assign_add(1)\n",
    "                opt._decayed_lr(tf.float32)\n",
    "                \n",
    "                #print(self.Loss)\n",
    "                #sys.exit(0)\n",
    "                if (i+1)%50==0:                    \n",
    "                    rep=(epoch*math.ceil(len_train_data/self.batch_size))+((i+1))\n",
    "                    txt='Epoch %2d: i  %2d  out of  %4d     loss=%2.5f' %(epoch, i*self.batch_size, len_train_data, loss_/(rep))\n",
    "                    logfile.write(txt+\"\\n\")\n",
    "                    print(txt)    \n",
    "            #opt._decayed_lr(tf.float32)\n",
    "            #print(self.W4)\n",
    "            #validate the results\n",
    "            print(\"\\n************\\nValidation started....\\n\")\n",
    "            val_loss=0\n",
    "            \n",
    "            for ii in range(math.ceil(val_len/self.batch_size)):\n",
    "                if ((ii+1)*self.batch_size)<val_len:\n",
    "                    start=ii*self.batch_size\n",
    "                    end=(ii+1)*self.batch_size\n",
    "                else:\n",
    "                    start=ii*self.batch_size\n",
    "                    end=val_len\n",
    "                \n",
    "                self.inputs= [self.val_data[start:end][i][0] for i in range(len(self.val_data[start:end]))]                            \n",
    "                self.qatextinput=[self.val_data_text[start:end][i][0] for i in range(len(self.val_data_text[start:end]))]\n",
    "                self.qaneighborinput=[self.val_data_neighbors[start:end][i][0] for i in range(len( self.val_data_neighbors[start:end]))]\n",
    "                \n",
    "                self.inputs_neg= [self.val_data[start:end][i][1] for i in range(len(self.val_data[start:end]))]                            \n",
    "                self.qatextinput_neg=[self.val_data_text[start:end][i][1] for i in range(len(self.val_data_text[start:end]))]\n",
    "                self.qaneighborinput_neg=[self.val_data_neighbors[start:end][i][1] for i in range(len( self.val_data_neighbors[start:end]))]\n",
    "                \n",
    "                \n",
    "                val_loss+=self.loss()\n",
    "                #print(self.loss())\n",
    "                #print(val_loss)\n",
    "                if (ii+1)%50==0:                   \n",
    "                    txt='Epoch %2d: ii  %2d  out of  %4d     validation loss=%2.5f' %(epoch, ii*self.batch_size, val_len, val_loss/(ii+1))\n",
    "                    logfile.write(txt+\"\\n\")\n",
    "                    print(txt)\n",
    "            txt='Epoch %2d: ii  %2d  out of  %4d     validation loss=%2.5f' %(epoch, ii*self.batch_size, val_len, val_loss/(ii+1))\n",
    "            logfile.write(txt+\"\\n\")\n",
    "            print(txt)\n",
    "            \n",
    "            if epoch%1==0:\n",
    "                pkl_filename =self.dataset+ \"/results/pickle_QR_model.pkl\"+str(epoch)+modelname\n",
    "                with open(pkl_filename, 'wb') as file:\n",
    "                    pickle.dump(self, file)\n",
    "                print(\"model was saved\")\n",
    "            t_loss.append(loss_/(rep))\n",
    "            v_loss.append(val_loss/math.ceil(val_len/self.batch_size))\n",
    "            eps.append(epoch)\n",
    "            plt.figure(figsize=(10,7))\n",
    "            plt.plot(eps,t_loss,'r-o',label = \"train loss\")\n",
    "            plt.plot(eps,v_loss,'b-*',label = \"validation loss\")\n",
    "            plt.title(\"train and validation losses\")\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.legend(loc=\"upper right\")\n",
    "            plt.savefig(self.dataset+ \"/results/loss.png\")\n",
    "            plt.show()\n",
    "        print(\"train model done!!\")\n",
    "        logfile.close() \n",
    "        #print(self.W4)\n",
    "        plt.figure(figsize=(10,7))\n",
    "        plt.plot(eps,t_loss,'r-o',label = \"train loss\")\n",
    "        plt.plot(eps,v_loss,'b-*',label = \"validation loss\")\n",
    "        plt.title(\"train and validation losses\")\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.savefig(self.dataset+ \"/results/loss.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    def test_model(dataset,modelname,path):        \n",
    "        pkl_filename =dataset+ \"/\"+path+modelname\n",
    "        # Load from file\n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            ob = pickle.load(file)\n",
    "        print(\"model was loaded!!\")\n",
    "        #print(regr.get_params(deep=True))        \n",
    "        #print(tf.reshape(ob.W4,(4,32)))\n",
    "        #sys.exit(0)\n",
    "        \n",
    "        INPUT=dataset+\"/test_data.txt\"\n",
    "        \n",
    "        fin_test=open(INPUT)        \n",
    "        test=fin_test.readline().strip()\n",
    "        test_data=[]\n",
    "        \n",
    "        while test:\n",
    "            data=test.split(\";\")\n",
    "            lst=[]\n",
    "            for d in data[0].split(\" \"):\n",
    "                lst.append(int(d)) \n",
    "            \n",
    "            alst=[]\n",
    "            \n",
    "            for d in data[1].split(\" \")[0::3]:\n",
    "                alst.append(int(d))\n",
    "            \n",
    "            anlst=[]\n",
    "            for d in data[1].split(\" \")[1::3]:\n",
    "                anlst.append(int(d))\n",
    "                \n",
    "            test_data.append([lst,alst,anlst])\n",
    "            \n",
    "            test=fin_test.readline().strip()\n",
    "        fin_test.close()       \n",
    "        \n",
    "        INPUT=dataset+\"/CQG_proporties.txt\"        \n",
    "        pfile=open(INPUT)\n",
    "        line=pfile.readline()\n",
    "        N=int(line.split(\" \")[2]) # number of nodes in the CQA network graph N=|Qestions|+|Askers|+|Answerers|+|tags|\n",
    "        line=pfile.readline()\n",
    "        qnum=int(line.split(\" \")[2])\n",
    "        \n",
    "                \n",
    "        user_id_map={}\n",
    "        INPUT3=dataset+\"/user_id_map.txt\"\n",
    "        fin=open(INPUT3, \"r\",encoding=\"utf8\")\n",
    "        line=fin.readline().strip()\n",
    "        while line:            \n",
    "            e=line.split(\" \")\n",
    "            uname=\" \".join(e[1:])            \n",
    "            uname=uname.strip()\n",
    "            user_id_map[uname]=qnum+int(e[0])            \n",
    "            line=fin.readline().strip()\n",
    "            \n",
    "        fin.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        answers={}\n",
    "        qtitle={}\n",
    "        qcontent={}\n",
    "        vocab=[]\n",
    "        \n",
    "        INPUT=dataset+\"/vocab.txt\"\n",
    "        fin=open( INPUT, \"r\")\n",
    "        line=fin.readline()\n",
    "        line=fin.readline().strip()\n",
    "        while line:\n",
    "            v = line.split(\" \")        \n",
    "            vocab.append(v[0])\n",
    "            line=fin.readline().strip()\n",
    "        \n",
    "        INPUT=dataset+\"/A_content_nsw.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                answers[int(d[0])]=d[1:]\n",
    "        \n",
    "        INPUT=dataset+\"/Q_content_nsw.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                qcontent[int(d[0])]=d[1:]\n",
    "        \n",
    "        INPUT=dataset+\"/Q_title_nsw.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                qtitle[int(d[0])]=d[1:] \n",
    "        \n",
    "        Q_id_map_to_original={}\n",
    "        INPUT2=dataset+\"/Q_id_map.txt\"\n",
    "        ids=np.loadtxt(INPUT2, dtype=int)\n",
    "        for e in ids:\n",
    "            Q_id_map_to_original[int(e[0])]=int(e[1])\n",
    "            \n",
    "        max_q_len=ob.max_q_len\n",
    "        ob.max_d_len=1*ob.max_d_len\n",
    "        max_d_len=ob.max_d_len\n",
    "        max_q_len=20\n",
    "        max_d_len=100\n",
    "        u_answers={}\n",
    "        INPUT=dataset+\"/user_answers.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                u_answers[int(d[0])]=d[1::2]\n",
    "                \n",
    "        \n",
    "        batch_size=1        \n",
    "        OUTPUT=dataset+\"/\"+path+\"test_results_\"+modelname+\".txt\"\n",
    "        fout=open(OUTPUT,\"w\")\n",
    "        #results=[]        \n",
    "        iii=0\n",
    "        for tq in test_data:\n",
    "            print(iii)\n",
    "            iii=iii+1\n",
    "            print(\"test q:\")\n",
    "            print(tq)            \n",
    "           \n",
    "            ids=tq[1] \n",
    "            answerids=tq[2]\n",
    "            \n",
    "            print(\"experts:\")      \n",
    "            print(ids)\n",
    "            inputs=[]\n",
    "            inputtext=[]\n",
    "            \n",
    "            qtext=[]\n",
    "            inputneighbors=[]\n",
    "            \n",
    "            qid=Q_id_map_to_original[int(tq[0][0])]\n",
    "            qtext1=qtitle[qid].copy()\n",
    "            qtext1.extend(qcontent[qid])\n",
    "            qtext1=qtext1[:20]\n",
    "            qtext=qtext1.copy()\n",
    "            #print(qtext)\n",
    "            for i in range(len(qtext)):\n",
    "                qtext[i]=vocab.index(qtext[i])+1\n",
    "            \n",
    "            #if len(qtext)<max_q_len:                \n",
    "            #        for i in range(max_q_len-len(qtext)):\n",
    "                        #qtext.append(0)\n",
    "            kkk=0\n",
    "            for e in ids:              \n",
    "                answerid=answerids[kkk]\n",
    "                kkk+=1\n",
    "                etext1=[]\n",
    "                if answerid!=-1:\n",
    "                    etext1=answers[int(answerid)][:100]\n",
    "                    etext=etext1\n",
    "                else:       \n",
    "                    for aid in u_answers[int(e)]:\n",
    "                        #print(aid)\n",
    "                        etext1.extend(answers[int(aid)][:100])\n",
    "                        #etext1.extend(answers[int(aid)])\n",
    "                    \n",
    "                    #print(\"inter\")\n",
    "                    #print(inter)\n",
    "                    etext=etext1\n",
    "                    #etext=etext1\n",
    "                    if len(etext1)>max_d_len:                         \n",
    "                            etext=random.sample(etext1,max_d_len)\n",
    "                        \n",
    "                \n",
    "                #print(etext)\n",
    "                \n",
    "                for ii in range(len(etext)):\n",
    "                    etext[ii]=vocab.index(etext[ii])+1\n",
    "                \n",
    "                #if len(etext)<max_d_len:                \n",
    "                    #for i in range(max_d_len-len(etext)):\n",
    "                        #etext.append(0)\n",
    "                \n",
    "                testlst=tq[0][0:2]\n",
    "                testlst.append(user_id_map[str(e)])\n",
    "                testlst=np.concatenate((testlst,tq[0][2:]))        \n",
    "                inputs.append(testlst)\n",
    "                inputtext.append([qtext,etext]) \n",
    "                \n",
    "                qid1=testlst[0]\n",
    "                #print(qid1)\n",
    "                #print(ob.neighbors)\n",
    "                answererid1=testlst[2]\n",
    "                qneighboirs=ob.neighbors[qid1].copy()\n",
    "                #qneighboirs.remove(answererid)\n",
    "                eneigbors=ob.neighbors[answererid1].copy()\n",
    "                \n",
    "                inputneighbors.append([qneighboirs,eneigbors])\n",
    "            ob.inputs=inputs\n",
    "            ob.qatextinput=inputtext\n",
    "            ob.qaneighborinput=inputneighbors\n",
    "            #print(ob.inputs)\n",
    "            #print(inputtext[0:2])\n",
    "            s=ob.model_test().numpy() \n",
    "            print(s)\n",
    "            res=\"\"\n",
    "            for i in range(len(ids)):\n",
    "                res+=str(ids[i])+\" \"+ str(s[i])+\";\" \n",
    "            \n",
    "            #res=\" \".join([str(r) for r in sorted_ids[0:topk]])\n",
    "            fout.write(res.strip()+\"\\n\")\n",
    "            fout.flush()\n",
    "        fout.close()\n",
    "        #OUTPUT=dataset+\"/ColdEndFormat/EndCold_test_results.txt\" \n",
    "        #np.savetxt(OUTPUT,np.array(results), fmt='%d')\n",
    "        print(\"test_model done!!\")           \n",
    "    \n",
    "    def test_model_allanswerers(dataset,modelname,path):        \n",
    "        pkl_filename =dataset+ \"/\"+path+modelname\n",
    "        # Load from file\n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            ob = pickle.load(file)\n",
    "        print(\"model was loaded!!\")\n",
    "        #print(regr.get_params(deep=True))        \n",
    "        #print(tf.reshape(ob.W4,(4,32)))\n",
    "        #sys.exit(0)\n",
    "        \n",
    "        INPUT=dataset+\"/\"+\"test_data.txt\"\n",
    "        \n",
    "        fin_test=open(INPUT)        \n",
    "        test=fin_test.readline().strip()\n",
    "        test_data=[]\n",
    "        \n",
    "        while test:\n",
    "            data=test.split(\";\")\n",
    "            lst=[]\n",
    "            for d in data[0].split(\" \"):\n",
    "                lst.append(int(d)) \n",
    "            \n",
    "            alst=[]\n",
    "            \n",
    "            for d in data[1].split(\" \")[0::3]:\n",
    "                alst.append(int(d))\n",
    "            \n",
    "            anlst=[]\n",
    "            for d in data[1].split(\" \")[1::3]:\n",
    "                anlst.append(int(d))\n",
    "                \n",
    "            test_data.append([lst,alst,anlst])\n",
    "            \n",
    "            test=fin_test.readline().strip()\n",
    "        fin_test.close()       \n",
    "        \n",
    "        INPUT=dataset+\"/CQG_proporties.txt\"        \n",
    "        pfile=open(INPUT)\n",
    "        line=pfile.readline()\n",
    "        N=int(line.split(\" \")[2]) # number of nodes in the CQA network graph N=|Qestions|+|Askers|+|Answerers|+|tags|\n",
    "        line=pfile.readline()\n",
    "        qnum=int(line.split(\" \")[2])\n",
    "        \n",
    "                \n",
    "        user_id_map={}\n",
    "        INPUT3=dataset+\"/user_id_map.txt\"\n",
    "        fin=open(INPUT3, \"r\",encoding=\"utf8\")\n",
    "        line=fin.readline().strip()\n",
    "        while line:            \n",
    "            e=line.split(\" \")\n",
    "            uname=\" \".join(e[1:])            \n",
    "            uname=uname.strip()\n",
    "            user_id_map[uname]=qnum+int(e[0])            \n",
    "            line=fin.readline().strip()\n",
    "            \n",
    "        fin.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        answers={}\n",
    "        qtitle={}\n",
    "        qcontent={}\n",
    "        vocab=[]\n",
    "        \n",
    "        INPUT=dataset+\"/vocab.txt\"\n",
    "        fin=open( INPUT, \"r\")\n",
    "        line=fin.readline()\n",
    "        line=fin.readline().strip()\n",
    "        while line:\n",
    "            v = line.split(\" \")        \n",
    "            vocab.append(v[0])\n",
    "            line=fin.readline().strip()\n",
    "        \n",
    "        INPUT=dataset+\"/A_content_nsw.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                answers[int(d[0])]=d[1:]\n",
    "        \n",
    "        INPUT=dataset+\"/Q_content_nsw.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                qcontent[int(d[0])]=d[1:]\n",
    "        \n",
    "        INPUT=dataset+\"/Q_title_nsw.txt\"\n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                qtitle[int(d[0])]=d[1:] \n",
    "        \n",
    "        Q_id_map_to_original={}\n",
    "        INPUT2=dataset+\"/Q_id_map.txt\"\n",
    "        ids=np.loadtxt(INPUT2, dtype=int)\n",
    "        for e in ids:\n",
    "            Q_id_map_to_original[int(e[0])]=int(e[1])\n",
    "            \n",
    "        \n",
    "        allanswererids=[]\n",
    "        INPUT=dataset+\"/user_tags.txt\"\n",
    "        fin=open(INPUT,\"r\")\n",
    "        line=fin.readline()#skip file header\n",
    "        line=fin.readline().strip()#read first line\n",
    "        while line:\n",
    "            allanswererids.append(int(line.split(\" \")[0]))\n",
    "            line=fin.readline().strip()\n",
    "        fin.close()\n",
    "        allanswererids=np.array(allanswererids)\n",
    "        \n",
    "        max_q_len=ob.max_q_len\n",
    "        ob.max_d_len=1*ob.max_d_len\n",
    "        max_d_len=ob.max_d_len\n",
    "        max_q_len=20\n",
    "        max_d_len=100\n",
    "        u_answers={}\n",
    "        INPUT=dataset+\"/user_answers.txt\"\n",
    "        \n",
    "        \n",
    "        with open( INPUT, \"r\") as fin:                \n",
    "            for line in fin:\n",
    "                d = line.strip().split(\" \")        \n",
    "                u_answers[int(d[0])]=d[1::2]\n",
    "                \n",
    "                \n",
    "        \n",
    "        batch_size=1        \n",
    "        OUTPUT=dataset+\"/\"+path+\"test_results_all_\"+modelname+\".txt\"\n",
    "        fout=open(OUTPUT,\"w\")\n",
    "        #results=[]        \n",
    "        iii=0\n",
    "        for tq in test_data:\n",
    "            print(iii)\n",
    "            iii=iii+1\n",
    "            print(\"test q:\")\n",
    "            print(tq)\n",
    "            \n",
    "            alleids=list(np.setdiff1d(allanswererids,tq[1]))\n",
    "            allaids=[-1]*len(alleids)\n",
    "            \n",
    "            ids=tq[1]\n",
    "            ids.extend(alleids)\n",
    "            answerids=tq[2]\n",
    "            answerids.extend(allaids)\n",
    "            \n",
    "            print(\"experts:\")      \n",
    "            #print(ids)\n",
    "            inputs=[]\n",
    "            inputtext=[]\n",
    "            inputneighbors=[]\n",
    "            qtext=[]\n",
    "            qid=Q_id_map_to_original[int(tq[0][0])]\n",
    "            qtext1=qtitle[qid].copy()\n",
    "            qtext1.extend(qcontent[qid])\n",
    "            qtext1=qtext1[:20]\n",
    "            qtext=qtext1.copy()\n",
    "            #print(qtext)\n",
    "            for i in range(len(qtext)):\n",
    "                qtext[i]=vocab.index(qtext[i])+1\n",
    "            \n",
    "            #if len(qtext)<max_q_len:                \n",
    "            #        for i in range(max_q_len-len(qtext)):\n",
    "                        #qtext.append(0)\n",
    "            kkk=0\n",
    "            for e in ids:              \n",
    "                answerid=answerids[kkk]\n",
    "                kkk+=1\n",
    "                etext1=[]\n",
    "                if answerid!=-1:\n",
    "                    etext1=answers[int(answerid)][:100]\n",
    "                    etext=etext1\n",
    "                else:       \n",
    "                    for aid in u_answers[int(e)]:\n",
    "                        #print(aid)\n",
    "                        etext1.extend(answers[int(aid)][:100])\n",
    "                        #etext1.extend(answers[int(aid)])\n",
    "                    \n",
    "                    #print(\"inter\")\n",
    "                    #print(inter)\n",
    "                    etext=etext1\n",
    "                    #etext=etext1\n",
    "                    if len(etext1)>max_d_len:                         \n",
    "                            etext=random.sample(etext1,max_d_len)\n",
    "                        \n",
    "                \n",
    "                #print(etext)\n",
    "                \n",
    "                for ii in range(len(etext)):\n",
    "                    etext[ii]=vocab.index(etext[ii])+1\n",
    "                \n",
    "                #if len(etext)<max_d_len:                \n",
    "                    #for i in range(max_d_len-len(etext)):\n",
    "                        #etext.append(0)\n",
    "                \n",
    "                testlst=tq[0][0:2]\n",
    "                testlst.append(user_id_map[str(e)])\n",
    "                testlst=np.concatenate((testlst,tq[0][2:]))        \n",
    "                inputs.append(testlst)\n",
    "                inputtext.append([qtext,etext]) \n",
    "                \n",
    "                qid1=testlst[0]\n",
    "                #print(qid1)\n",
    "                #print(ob.neighbors)\n",
    "                answererid1=testlst[2]\n",
    "                qneighboirs=ob.neighbors[qid1].copy()\n",
    "                #qneighboirs.remove(answererid)\n",
    "                eneigbors=ob.neighbors[answererid1].copy()\n",
    "                \n",
    "                inputneighbors.append([qneighboirs,eneigbors])\n",
    "            ob.inputs=inputs\n",
    "            ob.qatextinput=inputtext\n",
    "            ob.qaneighborinput=inputneighbors\n",
    "            \n",
    "                        \n",
    "            #print(ob.inputs)\n",
    "            #print(inputtext[0:2])\n",
    "            s=ob.model_test().numpy() \n",
    "            print(s)\n",
    "            res=\"\"\n",
    "            for i in range(len(ids)):\n",
    "                res+=str(ids[i])+\" \"+ str(s[i])+\";\" \n",
    "            \n",
    "            #res=\" \".join([str(r) for r in sorted_ids[0:topk]])\n",
    "            fout.write(res.strip()+\"\\n\")\n",
    "            fout.flush()\n",
    "        fout.close()\n",
    "        #OUTPUT=dataset+\"/ColdEndFormat/EndCold_test_results.txt\" \n",
    "        #np.savetxt(OUTPUT,np.array(results), fmt='%d')\n",
    "        print(\"test_model done!!\")    \n",
    "    \n",
    "dataset=[\"android\",\"history\",\"dba\",\"physics\"] \n",
    "data=\"../data/\"+dataset[0]\n",
    "\n",
    "#step 1\n",
    "trian=False\n",
    "if trian==True:\n",
    "    ob=QRouting(data)\n",
    "    ob.train(\"c\") \n",
    "else: \n",
    "    option=[\"answerers+negativesamples\",\"alluser\"]\n",
    "    #answerers+negativesamples: given test q, rank its true answerers plus some negative samples \n",
    "    #alluser: rank all experts given a test q\n",
    "    op=option[1]\n",
    "    if op==option[0]:\n",
    "        for i in range(2,3):\n",
    "             QRouting.test_model(data,\"pickle_QR_model.pkl\"+str(i)+\"c\",\"results/\")\n",
    "    elif op==option[1]:            \n",
    "        QRouting.test_model_allanswerers(data,\"pickle_QR_model.pkl0c\",\"results/\")\n",
    "print(\"Done!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
